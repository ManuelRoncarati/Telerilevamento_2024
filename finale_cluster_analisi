#Codice per i Dati SOM (senza campionamento, iterato su tutti i modelli)
# Carica le librerie necessarie
library(kohonen)    # Per la gestione della Self-Organizing Map
library(factoextra) # (Opzionale, per eventuali visualizzazioni)
library(hopkins)    # Per il calcolo della statistica di Hopkins

# Specifica la cartella contenente i modelli SOM
som_folder <- "C:/PN_Sila/som_Sila"

# Elenca tutti i file con estensione "_som_model.rds" nella cartella
som_files <- list.files(path = som_folder, pattern = "_som_model\\.rds$", full.names = TRUE)

# Crea un data frame per salvare i risultati
results_som <- data.frame(
  File = character(0),
  Hopkins_SOM = numeric(0),
  Hopkins_Random = numeric(0),
  stringsAsFactors = FALSE
)

# Itera su tutti i modelli SOM
for (file in som_files) {
  # Carica il modello SOM
  som_model <- readRDS(file)
  
  # Estrai i dati elaborati dalla SOM (ad esempio, i codebook vectors)
  som_values <- som_model$codes[[1]]
  
  # Calcola la statistica di Hopkins sull'intero dataset SOM
  hopkins_som <- hopkins(som_values)
  
  # Genera un dataset casuale con la stessa distribuzione per ogni colonna
  random_df <- as.data.frame(apply(som_values, 2, function(x) {
    runif(length(x), min = min(x), max = max(x))
  }))
  
  # Calcola la statistica di Hopkins sul dataset casuale
  hopkins_random <- hopkins(random_df)
  
  # Stampa i risultati per il file corrente
  cat("File:", basename(file), "\n")
  cat("  Hopkins statistic for SOM data:    ", hopkins_som, "\n")
  cat("  Hopkins statistic for Random data: ", hopkins_random, "\n\n")
  
  # Aggiunge i risultati al data frame
  results_som <- rbind(results_som, data.frame(
    File = basename(file),
    Hopkins_SOM = hopkins_som,
    Hopkins_Random = hopkins_random,
    stringsAsFactors = FALSE
  ))
}

# Stampa il riepilogo dei risultati per i modelli SOM
print(results_som)















#Codice per i Dati Grezzi (Raw) con Campionamento a 10.000 osservazioni
# Carica le librerie necessarie
library(terra)
library(hopkins)

# Specifica la cartella contenente le immagini .tif
image_dir <- "C:/PN_Sila"
image_files <- list.files(path = image_dir, pattern = "\\.tif$", full.names = TRUE)

# Crea un data frame per salvare i risultati
results_raw <- data.frame(
  File = character(0),
  Hopkins_Data = numeric(0),
  Hopkins_Random = numeric(0),
  stringsAsFactors = FALSE
)

# Itera su tutte le immagini nella cartella
for (file in image_files) {
  # Carica il raster multibanda
  raster_image <- rast(file)
  
  # Calcola gli indici:
  # NDVI = (NIR - Red) / (NIR + Red) (assumendo banda 4 = NIR, banda 3 = Red)
  ndvi <- (raster_image[[4]] - raster_image[[3]]) / (raster_image[[4]] + raster_image[[3]])
  
  # MNDWI = (Green - SWIR) / (Green + SWIR) (assumendo banda 2 = Green, banda 5 = SWIR)
  mndwi <- (raster_image[[2]] - raster_image[[5]]) / (raster_image[[2]] + raster_image[[5]])
  
  # NDBI = (SWIR - NIR) / (SWIR + NIR) (assumendo banda 5 = SWIR, banda 4 = NIR)
  ndbi <- (raster_image[[5]] - raster_image[[4]]) / (raster_image[[5]] + raster_image[[4]])
  
  # Crea uno stack contenente le bande originali ed i nuovi indici
  stacked_raster <- c(raster_image, ndvi, mndwi, ndbi)
  
  # Estrai i valori come matrice, rimuovi le righe contenenti NA
  matrix_values <- values(stacked_raster)
  cleaned_matrix <- matrix_values[complete.cases(matrix_values), ]
  
  # Normalizza ogni colonna (Min-Max normalization)
  normalized_matrix <- apply(cleaned_matrix, 2, function(x) {
    (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
  })
  
  # Campiona 10.000 righe (oppure usa tutti i dati se inferiori a 10.000)
  sample_size <- min(10000, nrow(normalized_matrix))
  sampled_matrix <- normalized_matrix[sample(nrow(normalized_matrix), sample_size), ]
  
  # Calcola la statistica di Hopkins sul campione dei dati grezzi
  hopkins_data <- hopkins(sampled_matrix)
  
  # Genera un dataset casuale con la stessa distribuzione per ogni colonna del campione
  random_df <- as.data.frame(apply(sampled_matrix, 2, function(x) {
    runif(length(x), min = min(x), max = max(x))
  }))
  
  # Calcola la statistica di Hopkins sul dataset casuale
  hopkins_random <- hopkins(random_df)
  
  # Stampa i risultati per il file corrente
  cat("File:", basename(file), "\n")
  cat("  Hopkins statistic for raw data:    ", hopkins_data, "\n")
  cat("  Hopkins statistic for Random data: ", hopkins_random, "\n\n")
  
  # Aggiunge i risultati al data frame
  results_raw <- rbind(results_raw, data.frame(
    File = basename(file),
    Hopkins_Data = hopkins_data,
    Hopkins_Random = hopkins_random,
    stringsAsFactors = FALSE
  ))
}

# Stampa il riepilogo dei risultati per i dati grezzi
print(results_raw)















#questa cosa ci permette di vedere se la distrubuzione dei dati addestrati dalla som è casuale o meno

# Caricamento delle librerie necessarie
library(kohonen)    # Libreria per le Self-Organizing Maps
library(factoextra) # Libreria per la visualizzazione dei risultati della PCA

# Caricamento del modello SOM
# Il modello SOM è stato precedentemente addestrato e salvato in un file .rds
# Il file viene caricato per estrarre i codici, che rappresentano i pesi dei neuroni della mappa
som_model <- readRDS("C:/PN_Sila/som_Sila/Composite_Filled_PN_Sila1984-06-15_1988-08-15.tif_som_model.rds")

# Estrazione dei codici della SOM, che corrispondono ai centroidi dei cluster
som_values <- som_model$codes[[1]]  

# Applicazione dell'analisi delle componenti principali ai dati della SOM
# Questo permette di ridurre la dimensionalità e visualizzare le strutture nei dati
fviz_pca_ind(prcomp(som_values), 
             title = "PCA - SOM Data",
             geom = "point", 
             ggtheme = theme_classic())

# Generazione di un dataset casuale con la stessa struttura dei dati della SOM
# Ogni colonna viene riempita con valori casuali estratti da una distribuzione uniforme
# con minimi e massimi corrispondenti a quelli della colonna originale
random_df <- apply(som_values, 2, function(x) runif(length(x), min(x), max(x)))
random_df <- as.data.frame(random_df)  

# Applicazione dell'analisi delle componenti principali ai dati casuali
fviz_pca_ind(prcomp(random_df), 
             title = "PCA - Random Data",
             geom = "point", 
             ggtheme = theme_classic())







#STESSA COSA MA SENZA SOM
# Carica le librerie necessarie
library(terra)
library(factoextra)  # Per la visualizzazione della PCA
library(ggplot2)     # Per salvare i plot

# Cartella in cui salvare i plot PCA (modifica il percorso se necessario)
plot_output_folder <- "C:/PN_Gennargentu/pca_plots"
if (!dir.exists(plot_output_folder)) {
    dir.create(plot_output_folder, recursive = TRUE)
}

# Misura il tempo di esecuzione dell'intero script
start_time <- Sys.time()

# Funzione per la normalizzazione Min-Max
normalize <- function(x) {
    (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# Funzione per elaborare una singola immagine e eseguire le verifiche PCA
process_image <- function(image_path) {
    # Carica il raster multibanda e calcola gli indici
    raster_image <- rast(image_path)
    ndvi  <- (raster_image[[4]] - raster_image[[3]]) / (raster_image[[4]] + raster_image[[3]])
    mndwi <- (raster_image[[2]] - raster_image[[5]]) / (raster_image[[2]] + raster_image[[5]])
    ndbi  <- (raster_image[[5]] - raster_image[[4]]) / (raster_image[[5]] + raster_image[[4]])
    
    # Crea lo stack con bande e indici
    stacked_raster <- c(raster_image, ndvi, mndwi, ndbi)
    
    # Estrai i valori come matrice, rimuovi NA e normalizza
    matrix_values <- values(stacked_raster)
    cleaned_matrix <- matrix_values[complete.cases(matrix_values), ]
    normalized_matrix <- apply(cleaned_matrix, 2, normalize)
    
    ## --- VERIFICHE SUI DATI ---
    # 1. PCA sui dati normalizzati
    pca_data <- prcomp(normalized_matrix)
    p1 <- fviz_pca_ind(pca_data, 
                       title = paste("PCA - Dati per", basename(image_path)),
                       geom = "point", 
                       ggtheme = theme_classic())
    
    # Salva il plot PCA sui dati originali
    plot_file_data <- file.path(plot_output_folder, paste0(basename(image_path), "_pca_data.png"))
    ggsave(plot_file_data, plot = p1, width = 7, height = 5)
    
    # 2. Genera un dataset casuale con la stessa struttura dei dati originali
    random_df <- as.data.frame(apply(normalized_matrix, 2, function(x) runif(length(x), min(x), max(x))))
    pca_random <- prcomp(random_df)
    p2 <- fviz_pca_ind(pca_random, 
                       title = paste("PCA - Dati Casuali per", basename(image_path)),
                       geom = "point", 
                       ggtheme = theme_classic())
    
    # Salva il plot PCA sui dati casuali
    plot_file_random <- file.path(plot_output_folder, paste0(basename(image_path), "_pca_random.png"))
    ggsave(plot_file_random, plot = p2, width = 7, height = 5)
    ## --- FINE VERIFICHE SUI DATI ---
}

# Ottieni il percorso della prima immagine nella cartella
image_paths <- list.files(path = "C:/PN_Gennargentu", pattern = "\\.tif$", full.names = TRUE)
if (length(image_paths) > 0) {
    process_image(image_paths[1])
} else {
    cat("Nessuna immagine trovata nella cartella.\n")
}

# Calcola e stampa il tempo totale di esecuzione
total_time <- Sys.time() - start_time
cat("Tempo totale di esecuzione:", total_time, "\n")









# dati fortemente clusterizzati

# Caricamento delle librerie necessarie
library(kohonen)    # Per la Self-Organizing Map
library(factoextra) # Per visualizzare la matrice di dissimilarità
library(hopkins)    # Per il calcolo della statistica di Hopkins

# Caricamento del modello SOM
som_model <- readRDS("C:/PN_Sila/som_Sila/Composite_Filled_PN_Sila1984-06-15_1988-08-15.tif_som_model.rds")

# Estrazione dei codici della SOM (matrice dei centroidi)
som_values <- som_model$codes[[1]]  

# Calcolo della statistica di Hopkins per i dati SOM, considerando tutti i punti
hopkins_som <- hopkins(som_values)
print(paste("Hopkins statistic for SOM data:", hopkins_som))

# Generazione di un dataset casuale con la stessa distribuzione dei dati SOM
random_df <- apply(som_values, 2, function(x) runif(length(x), min(x), max(x)))
random_df <- as.data.frame(random_df)

# Calcolo della statistica di Hopkins per il dataset casuale
hopkins_random <- hopkins(random_df)
print(paste("Hopkins statistic for Random data:", hopkins_random))

# Visualizzazione della matrice di dissimilarità per i dati della SOM
fviz_dist(dist(som_values), show_labels = FALSE) +
  labs(title = "Dissimilarity Matrix - SOM Data")

# Visualizzazione della matrice di dissimilarità per i dati casuali
fviz_dist(dist(random_df), show_labels = FALSE) +
  labs(title = "Dissimilarity Matrix - Random Data")







#STESSA COSA MA CON L'IMMAGINE SENZA SOM
# Carica le librerie necessarie
library(terra)
library(hopkins)

# Specifica il percorso del file immagine (modifica il percorso se necessario)
image_path <- "C:/PN_Sila/Composite_Filled_PN_Sila1984-06-15_1988-08-15.tif"

# Carica il raster multibanda
raster_image <- rast(image_path)

# Calcola gli indici: NDVI, MNDWI e NDBI
ndvi  <- (raster_image[[4]] - raster_image[[3]]) / (raster_image[[4]] + raster_image[[3]])
mndwi <- (raster_image[[2]] - raster_image[[5]]) / (raster_image[[2]] + raster_image[[5]])
ndbi  <- (raster_image[[5]] - raster_image[[4]]) / (raster_image[[5]] + raster_image[[4]])

# Crea lo stack contenente le bande originali e gli indici
stacked_raster <- c(raster_image, ndvi, mndwi, ndbi)

# Estrai i valori come matrice e rimuovi le righe contenenti NA
matrix_values  <- values(stacked_raster)
cleaned_matrix <- matrix_values[complete.cases(matrix_values), ]

# Funzione per la normalizzazione Min-Max
normalize <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# Normalizza i dati colonna per colonna
normalized_matrix <- apply(cleaned_matrix, 2, normalize)

# Stampa a video il numero totale di dati (righe) disponibili dopo la pulizia e normalizzazione
cat("Numero totale di dati (righe) dopo pulizia e normalizzazione:", nrow(normalized_matrix), "\n")

# Campionamento dei dati per evitare problemi di overflow/memoria
n_sample <- min(1000, nrow(normalized_matrix))  # Utilizza 1000 righe oppure meno se non disponibili
sampled_data <- normalized_matrix[sample(nrow(normalized_matrix), n_sample), ]

# Calcolo della statistica di Hopkins per i dati originali campionati
hopkins_data <- hopkins(sampled_data)
cat("Hopkins statistic for original data:", hopkins_data, "\n")

# Genera un dataset casuale con la stessa struttura dei dati originali:
random_df <- as.data.frame(apply(normalized_matrix, 2, function(x) runif(length(x), min(x), max(x))))
sampled_random <- random_df[sample(nrow(random_df), n_sample), ]

# Calcola la statistica di Hopkins per il dataset casuale campionato
hopkins_random <- hopkins(sampled_random)
cat("Hopkins statistic for random data:", hopkins_random, "\n")











#siluette reiterata elbow reiterato clus_gap reiterato con stampe tutti AGGIUNGE ITER.MAX E NSTART A TUTTI, aggiungere k.max per limitare, magari k.max=6

# Caricamento delle librerie necessarie
library(kohonen)    # Per la Self-Organizing Map
library(factoextra) # Per la visualizzazione dei metodi di selezione del numero ottimale di cluster
library(cluster)    # Per metriche di clustering

# Definizione della cartella contenente i modelli SOM
som_folder <- "C:/PN_Sila/som_Sila/"

# Ottieni tutti i file .rds presenti nella cartella
som_files <- list.files(som_folder, pattern = "\\.rds$", full.names = TRUE)

# Itera su tutti i file SOM presenti nella cartella
for (som_file in som_files) {
  
  # Stampa il nome del file in analisi
  cat("\nAnalizzando il file:", som_file, "\n")
  
  # Caricamento del modello SOM
  som_model <- readRDS(som_file)
  
  # Estrazione dei codici della SOM (matrice dei centroidi)
  som_values <- som_model$codes[[1]]
  
  # Metodo del gomito (Elbow method) senza linea di intercetta
  print(fviz_nbclust(som_values, kmeans, method = "wss") +
    labs(title = paste("Elbow Method -", basename(som_file)),
         subtitle = "Determining optimal clusters"))
  
  # Metodo della silhouette
  print(fviz_nbclust(som_values, kmeans, method = "silhouette") +
    labs(title = paste("Silhouette Method -", basename(som_file)),
         subtitle = "Evaluating cluster cohesion"))
  
  # Metodo della statistica Gap
  # Nota: b = 5 è usato per velocizzare i test, ma il valore raccomandato è 500 per una maggiore robustezza
  print(fviz_nbclust(som_values, kmeans, method = "gap_stat", nboot = 5) +
    labs(title = paste("Gap Statistic Method -", basename(som_file)),
         subtitle = "Determining optimal clusters"))
}













#NBCLUST ITERATOOO

# Caricamento delle librerie necessarie
library(kohonen)    # Per leggere i modelli SOM
library(NbClust)    # Per determinare il numero ottimale di cluster
library(dplyr)      # Per organizzare i risultati
library(openxlsx)   # Per salvare i risultati in Excel

# Definizione della directory contenente i modelli SOM
rds_dir <- "C:/PN_Sila/som_Sila/"

# Recupero di tutti i file .rds presenti nella cartella
rds_files <- list.files(path = rds_dir, pattern = "\\.rds$", full.names = TRUE)

# Analisi del numero ottimale di cluster per ogni file SOM
results <- lapply(rds_files, function(rds_path) {
    
    # Stampa del nome del file in analisi
    cat("\nAnalizzando il file:", basename(rds_path), "\n")
    
    # Caricamento del modello SOM
    som_model <- readRDS(rds_path)
    
    # Estrazione dei codici della SOM (matrice dei centroidi)
    som_values <- som_model$codes[[1]]
    
    # Calcolo del numero ottimale di cluster con NbClust
    nbclust_result <- NbClust(
        data = som_values, 
        distance = "euclidean", 
        min.nc = 3, 
        max.nc = 15, 
        method = "kmeans", 
        index = "all"
    )
    
    # Identificazione del numero ottimale di cluster con il maggior supporto tra gli indici
    cluster_counts <- table(nbclust_result$Best.nc[1, ])
    optimal_clusters <- as.numeric(names(which.max(cluster_counts)))
    support_count <- max(cluster_counts)

    # Estrazione del range di anni dal nome del file
    dates <- regmatches(basename(rds_path), gregexpr("\\d{4}", basename(rds_path)))[[1]]
    year_range <- if (length(dates) == 2) paste(dates[1], "-", dates[2]) else "Range non trovato"

    # Restituzione dei risultati principali in formato tabellare
    tibble(
        File = basename(rds_path),
        Year_Range = year_range,
        Optimal_Clusters = optimal_clusters,
        Support_Count = support_count
    )
})

# Creazione di un dataframe unificato con tutti i risultati
results_summary <- bind_rows(results)

# Visualizzazione della tabella dei risultati
print(results_summary)

# Salvataggio dei risultati in formato Excel
output_excel_path <- "C:/PN_Sila/nbclust_best_results.xlsx"
write.xlsx(results_summary, file = output_excel_path, rowNames = FALSE)

# Conferma del salvataggio
cat("\nRisultati salvati in:", output_excel_path, "\n")

















#cluserizzaizone kmeasn reiterata con aree e salvataggio aree xls

# Caricamento delle librerie necessarie
library(terra)
library(kohonen)
library(cluster)
library(leaflet)
library(viridisLite)
library(openxlsx)

# Definizione delle cartelle
tif_dir <- "C:/PN_Sila/"
rds_dir <- "C:/PN_Sila/som_Sila/"
output_dir <- "C:/PN_Sila/output_raster/"
excel_file <- "C:/PN_Sila/cluster_areas.xlsx"  # Nome file Excel

# Creazione della cartella di output se non esiste
if (!dir.exists(output_dir)) {
    dir.create(output_dir)
}

# Lista per memorizzare i risultati delle aree dei cluster
results_list <- list()

# Trova tutti i file .tif nella cartella
tif_files <- list.files(path = tif_dir, pattern = "\\.tif$", full.names = TRUE)

# Itera su tutti i file .tif trovati
for (tif_path in tif_files) {
    
    # Costruisce il percorso del file RDS corrispondente
    rds_name <- paste0(basename(tif_path), "_som_model.rds")
    rds_path <- file.path(rds_dir, rds_name)
    
    # Controlla se il file RDS esiste
    if (!file.exists(rds_path)) {
        cat("File RDS corrispondente non trovato per:", tif_path, "\n")
        next
    }
    
    cat("\nProcesso il file:", tif_path, "con il modello:", rds_path, "\n")
    
    # Estrazione delle date dal nome del file
    dates <- regmatches(basename(tif_path), gregexpr("\\d{4}-\\d{2}-\\d{2}", basename(tif_path)))[[1]]
    start_date <- ifelse(length(dates) >= 1, dates[1], "Data_iniziale")
    end_date <- ifelse(length(dates) >= 2, dates[2], "Data_finale")
    date_label <- paste(start_date, "-", end_date)  # Etichetta semplificata
    
    # Caricamento del raster e del modello SOM
    img <- rast(tif_path)  # Carica il raster
    som_model <- readRDS(rds_path)  # Carica il modello SOM
    som_values <- som_model$codes[[1]]  # Estrai i codici SOM
    
    # Set.seed per garantire risultati riproducibili
    set.seed(123)  
    
    # Clustering con K-means
    optimal_clusters <- 5  # Numero di cluster
    kmeans_result <- kmeans(som_values, centers = optimal_clusters, nstart = 100, iter.max = 500)
    cat("Clustering completato con", optimal_clusters, "cluster.\n")
    
    # Creazione del raster classificato
    classified_raster <- rast(img)  # Copia il raster originale per la classificazione
    values(classified_raster) <- NA  # Imposta tutti i valori a NA
    mask <- !is.na(values(img[[1]]))  # Maschera per i pixel validi
    values(classified_raster)[mask] <- kmeans_result$cluster[som_model$unit.classif]  # Assegna i cluster
    classified_raster <- classified_raster[[1]]  # Considera solo il primo layer
    
    # Calcolo delle aree dei cluster
    area_totale <- expanse(classified_raster, byValue = TRUE, unit = "km")
    area_totale <- area_totale[order(area_totale$value), ]
    
    # Creazione di una tabella con i dati formattati per l'Excel
    cluster_info <- data.frame(
        Date = date_label,
        Cluster = paste("Cluster", area_totale$value),  # Solo identificatore
        Area_km2 = round(area_totale$area, 2),  # Colonna separata con area numerica
        LULCC = NA  # Colonna vuota da compilare manualmente
    )
    
    # Aggiunta della tabella alla lista dei risultati
    results_list[[date_label]] <- cluster_info
    
    # Salvataggio del raster classificato
    output_file <- file.path(output_dir, paste0(basename(tif_path), "_classified.tif"))
    writeRaster(classified_raster, filename = output_file, overwrite = TRUE)
    cat("Raster classificato salvato in:", output_file, "\n")
    
    # Creazione della mappa Leaflet con legenda formattata
    col_pal <- viridisLite::viridis(length(area_totale$value))
    
    color_pal <- colorNumeric(
        palette = col_pal,
        domain = area_totale$value,
        na.color = "transparent"
    )
    
    legend_title <- paste0("Cluster (", date_label, ")")
    
    leaflet_map <- leaflet() %>%
        addTiles(group = "OpenStreetMap") %>%
        addProviderTiles("Esri.WorldImagery", group = "Immagine Satellitare") %>%
        addProviderTiles("CartoDB.Positron", group = "Mappa Chiara") %>%
        addRasterImage(
            classified_raster, 
            colors = color_pal, 
            opacity = 0.8,
            project = TRUE,
            group = "Raster Classificato"
        ) %>%
        addLegend(
            position = "bottomright",
            colors = col_pal,
            labels = paste("Cluster", area_totale$value,
                           "(", round(area_totale$area, 2), "km²)"),
            title = legend_title,
            opacity = 1
        ) %>%
        addLayersControl(
            baseGroups = c("OpenStreetMap", "Immagine Satellitare", "Mappa Chiara"),
            overlayGroups = c("Raster Classificato"),
            options = layersControlOptions(collapsed = FALSE),
            position = "topright"
        )
    
    # Visualizzazione della mappa con Leaflet
    print(leaflet_map)
    cat("Elaborazione completata per:", tif_path, "\n")
}

# Unione di tutti i risultati in un unico data frame
final_results <- do.call(rbind, results_list)

# Salvataggio del file Excel con openxlsx
write.xlsx(final_results, file = excel_file, rowNames = FALSE)
cat("File Excel salvato in:", excel_file, "\n")




















    
   








   
   




















#clusgap reiterato con grafico finale delle sovrapposizioni per i vari anni

# Caricamento delle librerie necessarie
library(kohonen)    # Per la Self-Organizing Map
library(factoextra) # Per la visualizzazione dei metodi di selezione del numero ottimale di cluster
library(cluster)    # Per metriche di clustering
library(ggplot2)    # Per la composizione dei grafici finali

# Definizione della cartella contenente i modelli SOM
som_folder <- "C:/PN_Sila/som_Sila/"

# Ottieni tutti i file .rds presenti nella cartella
som_files <- list.files(som_folder, pattern = "\\.rds$", full.names = TRUE)

# Inizializza le liste per raccogliere i dati dei vari metodi
elbow_data_list <- list()
sil_data_list   <- list()
gap_data_list   <- list()

# Parametri aggiuntivi per kmeans
kmax      <- 6    # Limite massimo di cluster
iter_max  <- 100  # Numero massimo di iterazioni
nstart    <- 10   # Numero di inizializzazioni casuali

# Ciclo su tutti i file SOM presenti nella cartella
for (som_file in som_files) {
    
    # Stampa il nome del file in analisi
    cat("\nAnalizzando il file:", som_file, "\n")
    
    # Caricamento del modello SOM
    som_model <- readRDS(som_file)
    
    # Estrazione dei codici della SOM (matrice dei centroidi)
    som_values <- som_model$codes[[1]]
    
    ### Metodo del gomito (Elbow method)
    elbow_plot <- fviz_nbclust(
        som_values, 
        kmeans, 
        method  = "wss", 
        k.max   = kmax, 
        iter.max= iter_max, 
        nstart  = nstart
    ) +
        labs(title    = paste("Elbow Method -", basename(som_file)),
             subtitle = "Determining optimal clusters")
    
    # Estrai i dati (solitamente in colonne "clusters" e "y") e aggiungi una colonna per identificare il file
    elbow_df <- elbow_plot$data
    elbow_df$File <- basename(som_file)
    elbow_data_list[[basename(som_file)]] <- elbow_df
    
    ### Metodo della silhouette
    sil_plot <- fviz_nbclust(
        som_values, 
        kmeans, 
        method  = "silhouette", 
        k.max   = kmax, 
        iter.max= iter_max, 
        nstart  = nstart
    ) +
        labs(title    = paste("Silhouette Method -", basename(som_file)),
             subtitle = "Evaluating cluster cohesion")
    
    sil_df <- sil_plot$data
    sil_df$File <- basename(som_file)
    sil_data_list[[basename(som_file)]] <- sil_df
    
    ### Metodo della statistica Gap
    # Nota: nboot rimane 5 per velocizzare i test (il valore raccomandato è 500 per robustezza)
    gap_plot <- fviz_nbclust(
        som_values, 
        kmeans, 
        method   = "gap_stat", 
        k.max    = kmax,
        nboot    = 500, 
        iter.max = iter_max, 
        nstart   = nstart
    ) +
        labs(title    = paste("Gap Statistic Method -", basename(som_file)),
             subtitle = "Determining optimal clusters")
    
    gap_df <- gap_plot$data
    gap_df$File <- basename(som_file)
    gap_data_list[[basename(som_file)]] <- gap_df
}

# Combina i dati raccolti in un unico data frame per ciascun metodo
elbow_data <- do.call(rbind, elbow_data_list)
sil_data   <- do.call(rbind, sil_data_list)
gap_data   <- do.call(rbind, gap_data_list)

# Creazione del grafico finale combinato per il metodo del gomito
p_elbow <- ggplot(elbow_data, aes(x = clusters, y = y, group = File, color = File)) +
    geom_line() +
    geom_point() +
    labs(title = "Elbow Method - Combinato",
         x     = "Numero di cluster",
         y     = "Total within-clusters sum of squares") +
    theme_minimal()

# Creazione del grafico finale combinato per il metodo della silhouette
p_sil <- ggplot(sil_data, aes(x = clusters, y = y, group = File, color = File)) +
    geom_line() +
    geom_point() +
    labs(title = "Silhouette Method - Combinato",
         x     = "Numero di cluster",
         y     = "Average silhouette width") +
    theme_minimal()

# Creazione del grafico finale combinato per il metodo della statistica Gap
# Nota: qui usiamo y = gap, in quanto il data frame di gap_stat contiene tale colonna
p_gap <- ggplot(gap_data, aes(x = clusters, y = gap, group = File, color = File)) +
    geom_line() +
    geom_point() +
    labs(title = "Gap Statistic Method - Combinato",
         x     = "Numero di cluster",
         y     = "Gap statistic") +
    theme_minimal()

# Visualizzazione dei tre grafici finali
print(p_elbow)
print(p_sil)
print(p_gap)












#indice di dunn tra 3 e 6 reiterato con sovrapposizione grafici
# Caricamento delle librerie necessarie
library(kohonen)    # Per la Self-Organizing Map
library(cluster)    # Per metriche di clustering
library(clValid)    # Per il calcolo dell'indice di Dunn
library(ggplot2)    # Per la composizione dei grafici finali

# Definizione della cartella contenente i modelli SOM
som_folder <- "C:/PN_Sila/som_Sila/"

# Ottieni tutti i file .rds presenti nella cartella
som_files <- list.files(som_folder, pattern = "\\.rds$", full.names = TRUE)

# Inizializza la lista per raccogliere i dati dell'indice di Dunn
dunn_data_list <- list()

# Parametri aggiuntivi per kmeans
kmax      <- 6  # Limite massimo di cluster
iter_max  <- 100  # Numero massimo di iterazioni
nstart    <- 25   # Numero di inizializzazioni casuali

# Ciclo su tutti i file SOM presenti nella cartella
for (som_file in som_files) {
    
    # Stampa il nome del file in analisi
    cat("\nAnalizzando il file:", som_file, "\n")
    
    # Caricamento del modello SOM
    som_model <- readRDS(som_file)
    
    # Estrazione dei codici della SOM (matrice dei centroidi)
    som_values <- som_model$codes[[1]]
    
    # Creazione di un data frame per memorizzare i valori di Dunn
    dunn_results <- data.frame(clusters = integer(), dunn_index = numeric())
    
    for (k in 3:kmax) {
        set.seed(123)  # Per risultati riproducibili
        kmeans_result <- kmeans(som_values, centers = k, iter.max = iter_max, nstart = nstart)
        dunn_index <- dunn(dist(som_values), kmeans_result$cluster)
        dunn_results <- rbind(dunn_results, data.frame(clusters = k, dunn_index = dunn_index))
    }
    
    dunn_results$File <- basename(som_file)
    dunn_data_list[[basename(som_file)]] <- dunn_results
}

# Combina i dati raccolti in un unico data frame
dunn_data <- do.call(rbind, dunn_data_list)

# Creazione del grafico finale combinato per l'indice di Dunn
p_dunn <- ggplot(dunn_data, aes(x = clusters, y = dunn_index, group = File, color = File)) +
    geom_line() +
    geom_point() +
    labs(title = "Dunn Index - Combinato",
         x     = "Numero di cluster",
         y     = "Dunn Index Value") +
    theme_minimal()

# Visualizzazione del grafico
print(p_dunn)

