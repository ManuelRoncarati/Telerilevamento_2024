#questa cosa ci permette di vedere se la distrubuzione dei dati addestrati dalla som è casuale o meno

# Caricamento delle librerie necessarie
library(kohonen)    # Libreria per le Self-Organizing Maps
library(factoextra) # Libreria per la visualizzazione dei risultati della PCA

# Caricamento del modello SOM
# Il modello SOM è stato precedentemente addestrato e salvato in un file .rds
# Il file viene caricato per estrarre i codici, che rappresentano i pesi dei neuroni della mappa
som_model <- readRDS("C:/PN_Sila/som_Sila/Composite_Filled_PN_Sila1984-06-15_1988-08-15.tif_som_model.rds")

# Estrazione dei codici della SOM, che corrispondono ai centroidi dei cluster
som_values <- som_model$codes[[1]]  

# Applicazione dell'analisi delle componenti principali ai dati della SOM
# Questo permette di ridurre la dimensionalità e visualizzare le strutture nei dati
fviz_pca_ind(prcomp(som_values), 
             title = "PCA - SOM Data",
             geom = "point", 
             ggtheme = theme_classic())

# Generazione di un dataset casuale con la stessa struttura dei dati della SOM
# Ogni colonna viene riempita con valori casuali estratti da una distribuzione uniforme
# con minimi e massimi corrispondenti a quelli della colonna originale
random_df <- apply(som_values, 2, function(x) runif(length(x), min(x), max(x)))
random_df <- as.data.frame(random_df)  

# Applicazione dell'analisi delle componenti principali ai dati casuali
fviz_pca_ind(prcomp(random_df), 
             title = "PCA - Random Data",
             geom = "point", 
             ggtheme = theme_classic())












# dati fortemente clusterizzati

# Caricamento delle librerie necessarie
library(kohonen)    # Per la Self-Organizing Map
library(factoextra) # Per visualizzare la matrice di dissimilarità
library(hopkins)    # Per il calcolo della statistica di Hopkins

# Caricamento del modello SOM
som_model <- readRDS("C:/PN_Sila/som_Sila/Composite_Filled_PN_Sila1984-06-15_1988-08-15.tif_som_model.rds")

# Estrazione dei codici della SOM (matrice dei centroidi)
som_values <- som_model$codes[[1]]  

# Calcolo della statistica di Hopkins per i dati SOM, considerando tutti i punti
hopkins_som <- hopkins(som_values)
print(paste("Hopkins statistic for SOM data:", hopkins_som))

# Generazione di un dataset casuale con la stessa distribuzione dei dati SOM
random_df <- apply(som_values, 2, function(x) runif(length(x), min(x), max(x)))
random_df <- as.data.frame(random_df)

# Calcolo della statistica di Hopkins per il dataset casuale
hopkins_random <- hopkins(random_df)
print(paste("Hopkins statistic for Random data:", hopkins_random))

# Visualizzazione della matrice di dissimilarità per i dati della SOM
fviz_dist(dist(som_values), show_labels = FALSE) +
  labs(title = "Dissimilarity Matrix - SOM Data")

# Visualizzazione della matrice di dissimilarità per i dati casuali
fviz_dist(dist(random_df), show_labels = FALSE) +
  labs(title = "Dissimilarity Matrix - Random Data")












#siluette reiterata elbow reiterato clus_gap reiterato con stampe tutti AGGIUNGE ITER.MAX E NSTART A TUTTI, aggiungere k.max per limitare, magari k.max=6

# Caricamento delle librerie necessarie
library(kohonen)    # Per la Self-Organizing Map
library(factoextra) # Per la visualizzazione dei metodi di selezione del numero ottimale di cluster
library(cluster)    # Per metriche di clustering

# Definizione della cartella contenente i modelli SOM
som_folder <- "C:/PN_Sila/som_Sila/"

# Ottieni tutti i file .rds presenti nella cartella
som_files <- list.files(som_folder, pattern = "\\.rds$", full.names = TRUE)

# Itera su tutti i file SOM presenti nella cartella
for (som_file in som_files) {
  
  # Stampa il nome del file in analisi
  cat("\nAnalizzando il file:", som_file, "\n")
  
  # Caricamento del modello SOM
  som_model <- readRDS(som_file)
  
  # Estrazione dei codici della SOM (matrice dei centroidi)
  som_values <- som_model$codes[[1]]
  
  # Metodo del gomito (Elbow method) senza linea di intercetta
  print(fviz_nbclust(som_values, kmeans, method = "wss") +
    labs(title = paste("Elbow Method -", basename(som_file)),
         subtitle = "Determining optimal clusters"))
  
  # Metodo della silhouette
  print(fviz_nbclust(som_values, kmeans, method = "silhouette") +
    labs(title = paste("Silhouette Method -", basename(som_file)),
         subtitle = "Evaluating cluster cohesion"))
  
  # Metodo della statistica Gap
  # Nota: b = 5 è usato per velocizzare i test, ma il valore raccomandato è 500 per una maggiore robustezza
  print(fviz_nbclust(som_values, kmeans, method = "gap_stat", nboot = 5) +
    labs(title = paste("Gap Statistic Method -", basename(som_file)),
         subtitle = "Determining optimal clusters"))
}













#NBCLUST ITERATOOO

# Caricamento delle librerie necessarie
library(kohonen)    # Per leggere i modelli SOM
library(NbClust)    # Per determinare il numero ottimale di cluster
library(dplyr)      # Per organizzare i risultati
library(openxlsx)   # Per salvare i risultati in Excel

# Definizione della directory contenente i modelli SOM
rds_dir <- "C:/PN_Sila/som_Sila/"

# Recupero di tutti i file .rds presenti nella cartella
rds_files <- list.files(path = rds_dir, pattern = "\\.rds$", full.names = TRUE)

# Analisi del numero ottimale di cluster per ogni file SOM
results <- lapply(rds_files, function(rds_path) {
    
    # Stampa del nome del file in analisi
    cat("\nAnalizzando il file:", basename(rds_path), "\n")
    
    # Caricamento del modello SOM
    som_model <- readRDS(rds_path)
    
    # Estrazione dei codici della SOM (matrice dei centroidi)
    som_values <- som_model$codes[[1]]
    
    # Calcolo del numero ottimale di cluster con NbClust
    nbclust_result <- NbClust(
        data = som_values, 
        distance = "euclidean", 
        min.nc = 3, 
        max.nc = 15, 
        method = "kmeans", 
        index = "all"
    )
    
    # Identificazione del numero ottimale di cluster con il maggior supporto tra gli indici
    cluster_counts <- table(nbclust_result$Best.nc[1, ])
    optimal_clusters <- as.numeric(names(which.max(cluster_counts)))
    support_count <- max(cluster_counts)

    # Estrazione del range di anni dal nome del file
    dates <- regmatches(basename(rds_path), gregexpr("\\d{4}", basename(rds_path)))[[1]]
    year_range <- if (length(dates) == 2) paste(dates[1], "-", dates[2]) else "Range non trovato"

    # Restituzione dei risultati principali in formato tabellare
    tibble(
        File = basename(rds_path),
        Year_Range = year_range,
        Optimal_Clusters = optimal_clusters,
        Support_Count = support_count
    )
})

# Creazione di un dataframe unificato con tutti i risultati
results_summary <- bind_rows(results)

# Visualizzazione della tabella dei risultati
print(results_summary)

# Salvataggio dei risultati in formato Excel
output_excel_path <- "C:/PN_Sila/nbclust_best_results.xlsx"
write.xlsx(results_summary, file = output_excel_path, rowNames = FALSE)

# Conferma del salvataggio
cat("\nRisultati salvati in:", output_excel_path, "\n")





