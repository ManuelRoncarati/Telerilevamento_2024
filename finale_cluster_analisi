#questa cosa ci permette di vedere se la distrubuzione dei dati addestrati dalla som è casuale o meno

# Caricamento delle librerie necessarie
library(kohonen)    # Libreria per le Self-Organizing Maps
library(factoextra) # Libreria per la visualizzazione dei risultati della PCA

# Caricamento del modello SOM
# Il modello SOM è stato precedentemente addestrato e salvato in un file .rds
# Il file viene caricato per estrarre i codici, che rappresentano i pesi dei neuroni della mappa
som_model <- readRDS("C:/PN_Sila/som_Sila/Composite_Filled_PN_Sila1984-06-15_1988-08-15.tif_som_model.rds")

# Estrazione dei codici della SOM, che corrispondono ai centroidi dei cluster
som_values <- som_model$codes[[1]]  

# Applicazione dell'analisi delle componenti principali ai dati della SOM
# Questo permette di ridurre la dimensionalità e visualizzare le strutture nei dati
fviz_pca_ind(prcomp(som_values), 
             title = "PCA - SOM Data",
             geom = "point", 
             ggtheme = theme_classic())

# Generazione di un dataset casuale con la stessa struttura dei dati della SOM
# Ogni colonna viene riempita con valori casuali estratti da una distribuzione uniforme
# con minimi e massimi corrispondenti a quelli della colonna originale
random_df <- apply(som_values, 2, function(x) runif(length(x), min(x), max(x)))
random_df <- as.data.frame(random_df)  

# Applicazione dell'analisi delle componenti principali ai dati casuali
fviz_pca_ind(prcomp(random_df), 
             title = "PCA - Random Data",
             geom = "point", 
             ggtheme = theme_classic())












# dati fortemente clusterizzati

# Caricamento delle librerie necessarie
library(kohonen)    # Per la Self-Organizing Map
library(factoextra) # Per visualizzare la matrice di dissimilarità
library(hopkins)    # Per il calcolo della statistica di Hopkins

# Caricamento del modello SOM
som_model <- readRDS("C:/PN_Sila/som_Sila/Composite_Filled_PN_Sila1984-06-15_1988-08-15.tif_som_model.rds")

# Estrazione dei codici della SOM (matrice dei centroidi)
som_values <- som_model$codes[[1]]  

# Calcolo della statistica di Hopkins per i dati SOM, considerando tutti i punti
hopkins_som <- hopkins(som_values)
print(paste("Hopkins statistic for SOM data:", hopkins_som))

# Generazione di un dataset casuale con la stessa distribuzione dei dati SOM
random_df <- apply(som_values, 2, function(x) runif(length(x), min(x), max(x)))
random_df <- as.data.frame(random_df)

# Calcolo della statistica di Hopkins per il dataset casuale
hopkins_random <- hopkins(random_df)
print(paste("Hopkins statistic for Random data:", hopkins_random))

# Visualizzazione della matrice di dissimilarità per i dati della SOM
fviz_dist(dist(som_values), show_labels = FALSE) +
  labs(title = "Dissimilarity Matrix - SOM Data")

# Visualizzazione della matrice di dissimilarità per i dati casuali
fviz_dist(dist(random_df), show_labels = FALSE) +
  labs(title = "Dissimilarity Matrix - Random Data")












#siluette reiterata elbow reiterato clus_gap reiterato con stampe tutti AGGIUNGE ITER.MAX E NSTART A TUTTI, aggiungere k.max per limitare, magari k.max=6

# Caricamento delle librerie necessarie
library(kohonen)    # Per la Self-Organizing Map
library(factoextra) # Per la visualizzazione dei metodi di selezione del numero ottimale di cluster
library(cluster)    # Per metriche di clustering

# Definizione della cartella contenente i modelli SOM
som_folder <- "C:/PN_Sila/som_Sila/"

# Ottieni tutti i file .rds presenti nella cartella
som_files <- list.files(som_folder, pattern = "\\.rds$", full.names = TRUE)

# Itera su tutti i file SOM presenti nella cartella
for (som_file in som_files) {
  
  # Stampa il nome del file in analisi
  cat("\nAnalizzando il file:", som_file, "\n")
  
  # Caricamento del modello SOM
  som_model <- readRDS(som_file)
  
  # Estrazione dei codici della SOM (matrice dei centroidi)
  som_values <- som_model$codes[[1]]
  
  # Metodo del gomito (Elbow method) senza linea di intercetta
  print(fviz_nbclust(som_values, kmeans, method = "wss") +
    labs(title = paste("Elbow Method -", basename(som_file)),
         subtitle = "Determining optimal clusters"))
  
  # Metodo della silhouette
  print(fviz_nbclust(som_values, kmeans, method = "silhouette") +
    labs(title = paste("Silhouette Method -", basename(som_file)),
         subtitle = "Evaluating cluster cohesion"))
  
  # Metodo della statistica Gap
  # Nota: b = 5 è usato per velocizzare i test, ma il valore raccomandato è 500 per una maggiore robustezza
  print(fviz_nbclust(som_values, kmeans, method = "gap_stat", nboot = 5) +
    labs(title = paste("Gap Statistic Method -", basename(som_file)),
         subtitle = "Determining optimal clusters"))
}













#NBCLUST ITERATOOO

# Caricamento delle librerie necessarie
library(kohonen)    # Per leggere i modelli SOM
library(NbClust)    # Per determinare il numero ottimale di cluster
library(dplyr)      # Per organizzare i risultati
library(openxlsx)   # Per salvare i risultati in Excel

# Definizione della directory contenente i modelli SOM
rds_dir <- "C:/PN_Sila/som_Sila/"

# Recupero di tutti i file .rds presenti nella cartella
rds_files <- list.files(path = rds_dir, pattern = "\\.rds$", full.names = TRUE)

# Analisi del numero ottimale di cluster per ogni file SOM
results <- lapply(rds_files, function(rds_path) {
    
    # Stampa del nome del file in analisi
    cat("\nAnalizzando il file:", basename(rds_path), "\n")
    
    # Caricamento del modello SOM
    som_model <- readRDS(rds_path)
    
    # Estrazione dei codici della SOM (matrice dei centroidi)
    som_values <- som_model$codes[[1]]
    
    # Calcolo del numero ottimale di cluster con NbClust
    nbclust_result <- NbClust(
        data = som_values, 
        distance = "euclidean", 
        min.nc = 3, 
        max.nc = 15, 
        method = "kmeans", 
        index = "all"
    )
    
    # Identificazione del numero ottimale di cluster con il maggior supporto tra gli indici
    cluster_counts <- table(nbclust_result$Best.nc[1, ])
    optimal_clusters <- as.numeric(names(which.max(cluster_counts)))
    support_count <- max(cluster_counts)

    # Estrazione del range di anni dal nome del file
    dates <- regmatches(basename(rds_path), gregexpr("\\d{4}", basename(rds_path)))[[1]]
    year_range <- if (length(dates) == 2) paste(dates[1], "-", dates[2]) else "Range non trovato"

    # Restituzione dei risultati principali in formato tabellare
    tibble(
        File = basename(rds_path),
        Year_Range = year_range,
        Optimal_Clusters = optimal_clusters,
        Support_Count = support_count
    )
})

# Creazione di un dataframe unificato con tutti i risultati
results_summary <- bind_rows(results)

# Visualizzazione della tabella dei risultati
print(results_summary)

# Salvataggio dei risultati in formato Excel
output_excel_path <- "C:/PN_Sila/nbclust_best_results.xlsx"
write.xlsx(results_summary, file = output_excel_path, rowNames = FALSE)

# Conferma del salvataggio
cat("\nRisultati salvati in:", output_excel_path, "\n")

















#cluserizzaizone kmeasn reiterata con aree e salvataggio aree xls

# Caricamento delle librerie necessarie
library(terra)
library(kohonen)
library(cluster)
library(leaflet)
library(viridisLite)
library(openxlsx)

# Definizione delle cartelle
tif_dir <- "C:/PN_Sila/"
rds_dir <- "C:/PN_Sila/som_Sila/"
output_dir <- "C:/PN_Sila/output_raster/"
excel_file <- "C:/PN_Sila/cluster_areas.xlsx"  # Nome file Excel

# Creazione della cartella di output se non esiste
if (!dir.exists(output_dir)) {
    dir.create(output_dir)
}

# Lista per memorizzare i risultati delle aree dei cluster
results_list <- list()

# Trova tutti i file .tif nella cartella
tif_files <- list.files(path = tif_dir, pattern = "\\.tif$", full.names = TRUE)

# Itera su tutti i file .tif trovati
for (tif_path in tif_files) {
    
    # Costruisce il percorso del file RDS corrispondente
    rds_name <- paste0(basename(tif_path), "_som_model.rds")
    rds_path <- file.path(rds_dir, rds_name)
    
    # Controlla se il file RDS esiste
    if (!file.exists(rds_path)) {
        cat("File RDS corrispondente non trovato per:", tif_path, "\n")
        next
    }
    
    cat("\nProcesso il file:", tif_path, "con il modello:", rds_path, "\n")
    
    # Estrazione delle date dal nome del file
    dates <- regmatches(basename(tif_path), gregexpr("\\d{4}-\\d{2}-\\d{2}", basename(tif_path)))[[1]]
    start_date <- ifelse(length(dates) >= 1, dates[1], "Data_iniziale")
    end_date <- ifelse(length(dates) >= 2, dates[2], "Data_finale")
    date_label <- paste(start_date, "-", end_date)  # Etichetta semplificata
    
    # Caricamento del raster e del modello SOM
    img <- rast(tif_path)  # Carica il raster
    som_model <- readRDS(rds_path)  # Carica il modello SOM
    som_values <- som_model$codes[[1]]  # Estrai i codici SOM
    
    # Set.seed per garantire risultati riproducibili
    set.seed(1234)  
    
    # Clustering con K-means
    optimal_clusters <- 5  # Numero di cluster
    kmeans_result <- kmeans(som_values, centers = optimal_clusters, nstart = 100, iter.max = 500)
    cat("Clustering completato con", optimal_clusters, "cluster.\n")
    
    # Creazione del raster classificato
    classified_raster <- rast(img)  # Copia il raster originale per la classificazione
    values(classified_raster) <- NA  # Imposta tutti i valori a NA
    mask <- !is.na(values(img[[1]]))  # Maschera per i pixel validi
    values(classified_raster)[mask] <- kmeans_result$cluster[som_model$unit.classif]  # Assegna i cluster
    classified_raster <- classified_raster[[1]]  # Considera solo il primo layer
    
    # Calcolo delle aree dei cluster
    area_totale <- expanse(classified_raster, byValue = TRUE, unit = "km")
    area_totale <- area_totale[order(area_totale$value), ]
    
    # Creazione di una tabella con i dati formattati per l'Excel
    cluster_info <- data.frame(
        Date = date_label,
        Cluster = paste("Cluster", area_totale$value),  # Solo identificatore
        Area_km2 = round(area_totale$area, 2),  # Colonna separata con area numerica
        LULCC = NA  # Colonna vuota da compilare manualmente
    )
    
    # Aggiunta della tabella alla lista dei risultati
    results_list[[date_label]] <- cluster_info
    
    # Salvataggio del raster classificato
    output_file <- file.path(output_dir, paste0(basename(tif_path), "_classified.tif"))
    writeRaster(classified_raster, filename = output_file, overwrite = TRUE)
    cat("Raster classificato salvato in:", output_file, "\n")
    
    # Creazione della mappa Leaflet con legenda formattata
    col_pal <- viridisLite::turbo(length(area_totale$value))
    
    color_pal <- colorNumeric(
        palette = col_pal,
        domain = area_totale$value,
        na.color = "transparent"
    )
    
    legend_title <- paste0("Cluster (", date_label, ")")
    
    leaflet_map <- leaflet() %>%
        addTiles(group = "OpenStreetMap") %>%
        addProviderTiles("Esri.WorldImagery", group = "Immagine Satellitare") %>%
        addProviderTiles("CartoDB.Positron", group = "Mappa Chiara") %>%
        addRasterImage(
            classified_raster, 
            colors = color_pal, 
            opacity = 0.8,
            project = TRUE,
            group = "Raster Classificato"
        ) %>%
        addLegend(
            position = "bottomright",
            colors = col_pal,
            labels = paste("Cluster", area_totale$value,
                           "(", round(area_totale$area, 2), "km²)"),
            title = legend_title,
            opacity = 1
        ) %>%
        addLayersControl(
            baseGroups = c("OpenStreetMap", "Immagine Satellitare", "Mappa Chiara"),
            overlayGroups = c("Raster Classificato"),
            options = layersControlOptions(collapsed = FALSE),
            position = "topright"
        )
    
    # Visualizzazione della mappa con Leaflet
    print(leaflet_map)
    cat("Elaborazione completata per:", tif_path, "\n")
}

# Unione di tutti i risultati in un unico data frame
final_results <- do.call(rbind, results_list)

# Salvataggio del file Excel con openxlsx
write.xlsx(final_results, file = excel_file, rowNames = FALSE)
cat("File Excel salvato in:", excel_file, "\n")



















#nselectboot
#clusterboot semplice semplice
# ====================================================
# Sezione 1: Caricamento delle librerie
# ====================================================
library(kohonen)    # Per leggere i modelli SOM
library(fpc)        # Per selezione del numero di cluster con bootstrap
library(tibble)     # Per organizzare i risultati
library(openxlsx)   # Per salvare i risultati in Excel
library(dplyr)      # Per bind_rows

# ====================================================
# Sezione 2: Impostazioni iniziali
# ====================================================
rds_dir <- "C:/Aspromonte/som_aspromonte"  # Directory dei file SOM
rds_files <- list.files(path = rds_dir, pattern = "\\.rds$", full.names = TRUE)  # Trova i file RDS

# ====================================================
# Sezione 3: Selezione del numero di cluster per ogni SOM
# ====================================================
results <- lapply(rds_files, function(rds_path) {
    cat("\nProcesso il file:", basename(rds_path), "\n")
    som_model <- readRDS(rds_path)  # Carica il modello SOM
    som_values <- som_model$codes[[1]]  # Estrai le codebook
    
    # Converte som_values in matrice se necessario
    som_matrix <- as.matrix(som_values)
    
    # Esegui il bootstrap per la selezione del numero di cluster con K-means
    result_som_kmeans <- nselectboot(
        data = som_matrix,                  # Matrice dei valori SOM
        B = 50,                             # Numero di campioni bootstrap
        clustermethod = kmeansCBI,          # Metodo di clustering K-means
        classification = "centroid",        # Metodo di classificazione
        krange = 2:10,                      # Numero di cluster da testare (da 2 a 10)
        count = FALSE                        # Stampa i numeri di cluster e le esecuzioni bootstrap
    )
    
    # Estrai il numero ottimale di cluster
    optimal_clusters <- result_som_kmeans$kopt
    
    # Estrai il range di anni dal nome del file
    dates <- regmatches(basename(rds_path), gregexpr("\\d{4}", basename(rds_path)))[[1]]
    year_range <- if (length(dates) == 2) paste(dates[1], "-", dates[2]) else "range non trovato"
    
    # Ritorna il risultato
    tibble(File = basename(rds_path), Year_Range = year_range, Optimal_Clusters = optimal_clusters)
})

# ====================================================
# Sezione 4: Risultati finali
# ====================================================
results_summary <- bind_rows(results)  # Combina i risultati in un'unica tabella
print(results_summary)

# ====================================================
# Sezione 5: Salvataggio in Excel
# ====================================================
output_excel_path <- "C:/Aspromonte/cluster_selection_results.xlsx"
write.xlsx(results_summary, file = output_excel_path, rowNames = FALSE)
cat("\nRisultati salvati in formato Excel:", output_excel_path, "\n")









#prediction streght
#clusterboot semplice semplice
# ====================================================
# Sezione 1: Caricamento delle librerie
# ====================================================
library(kohonen)    # Per leggere i modelli SOM
library(fpc)        # Per selezione del numero di cluster con bootstrap
library(tibble)     # Per organizzare i risultati
library(openxlsx)   # Per salvare i risultati in Excel
library(dplyr)      # Per bind_rows

# ====================================================
# Sezione 2: Impostazioni iniziali
# ====================================================
rds_dir <- "C:/Aspromonte/som_aspromonte"  # Directory dei file SOM
rds_files <- list.files(path = rds_dir, pattern = "\\.rds$", full.names = TRUE)  # Trova i file RDS

# ====================================================
# Sezione 3: Selezione del numero di cluster per ogni SOM
# ====================================================
results <- lapply(rds_files, function(rds_path) {
    cat("\nProcesso il file:", basename(rds_path), "\n")
    som_model <- readRDS(rds_path)  # Carica il modello SOM
    som_values <- som_model$codes[[1]]  # Estrai le codebook

    # Converte som_values in matrice se necessario
    som_matrix <- as.matrix(som_values)

    # Esegui il bootstrap per la selezione del numero di cluster con K-means
    result_som_kmeans <- nselectboot(
        data = som_matrix,                  # Matrice dei valori SOM
        B = 50,                             # Numero di campioni bootstrap
        clustermethod = kmeansCBI,          # Metodo di clustering K-means
        classification = "centroid",        # Metodo di classificazione
        krange = 2:10,                      # Numero di cluster da testare (da 2 a 10)
        count = FALSE                        # Stampa i numeri di cluster e le esecuzioni bootstrap
    )

    # Estrai il numero ottimale di cluster
    optimal_clusters <- result_som_kmeans$kopt
    cat("Numero ottimale di cluster secondo nselectboot:", optimal_clusters, "\n")
    
    # Esegui prediction.strength per confermare il numero ottimale di cluster
    ps_kmeans <- prediction.strength(
        xdata = som_matrix,
        Gmin = 2,                 # Numero minimo di cluster da testare
        Gmax = 10,                # Numero massimo di cluster da testare
        M = 50,                   # Numero di ripetizioni
        clustermethod = kmeansCBI,
        classification = "centroid",
        cutoff = 0.8,             # Soglia per la prediction strength
        distances = FALSE,
        count = FALSE
    )
    
    cat("Numero ottimale di cluster secondo prediction.strength (K-means):", ps_kmeans$optimalk, "\n")
    cat("Valori di Prediction Strength per ciascun cluster:", ps_kmeans$mean.pred, "\n")
    
    # Estrai il range di anni dal nome del file
    dates <- regmatches(basename(rds_path), gregexpr("\\d{4}", basename(rds_path)))[[1]]
    year_range <- if (length(dates) == 2) paste(dates[1], "-", dates[2]) else "range non trovato"

    # Ritorna il risultato
    tibble(File = basename(rds_path), Year_Range = year_range, Optimal_Clusters = optimal_clusters, Prediction_Strength_Clusters = ps_kmeans$optimalk)
})

# ====================================================
# Sezione 4: Risultati finali
# ====================================================
results_summary <- bind_rows(results)  # Combina i risultati in un'unica tabella
print(results_summary)

# ====================================================
# Sezione 5: Salvataggio in Excel
# ====================================================
output_excel_path <- "C:/Aspromonte/cluster_selection_results.xlsx"
write.xlsx(results_summary, file = output_excel_path, rowNames = FALSE)
cat("\nRisultati salvati in formato Excel:", output_excel_path, "\n")





















#clusgap reiterato con grafico finale delle sovrapposizioni per i vari anni

# Caricamento delle librerie necessarie
library(kohonen)    # Per la Self-Organizing Map
library(factoextra) # Per la visualizzazione dei metodi di selezione del numero ottimale di cluster
library(cluster)    # Per metriche di clustering
library(ggplot2)    # Per la composizione dei grafici finali

# Definizione della cartella contenente i modelli SOM
som_folder <- "C:/PN_Sila/som_Sila/"

# Ottieni tutti i file .rds presenti nella cartella
som_files <- list.files(som_folder, pattern = "\\.rds$", full.names = TRUE)

# Inizializza le liste per raccogliere i dati dei vari metodi
elbow_data_list <- list()
sil_data_list   <- list()
gap_data_list   <- list()

# Parametri aggiuntivi per kmeans
kmax      <- 6    # Limite massimo di cluster
iter_max  <- 100  # Numero massimo di iterazioni
nstart    <- 10   # Numero di inizializzazioni casuali

# Ciclo su tutti i file SOM presenti nella cartella
for (som_file in som_files) {
    
    # Stampa il nome del file in analisi
    cat("\nAnalizzando il file:", som_file, "\n")
    
    # Caricamento del modello SOM
    som_model <- readRDS(som_file)
    
    # Estrazione dei codici della SOM (matrice dei centroidi)
    som_values <- som_model$codes[[1]]
    
    ### Metodo del gomito (Elbow method)
    elbow_plot <- fviz_nbclust(
        som_values, 
        kmeans, 
        method  = "wss", 
        k.max   = kmax, 
        iter.max= iter_max, 
        nstart  = nstart
    ) +
        labs(title    = paste("Elbow Method -", basename(som_file)),
             subtitle = "Determining optimal clusters")
    
    # Estrai i dati (solitamente in colonne "clusters" e "y") e aggiungi una colonna per identificare il file
    elbow_df <- elbow_plot$data
    elbow_df$File <- basename(som_file)
    elbow_data_list[[basename(som_file)]] <- elbow_df
    
    ### Metodo della silhouette
    sil_plot <- fviz_nbclust(
        som_values, 
        kmeans, 
        method  = "silhouette", 
        k.max   = kmax, 
        iter.max= iter_max, 
        nstart  = nstart
    ) +
        labs(title    = paste("Silhouette Method -", basename(som_file)),
             subtitle = "Evaluating cluster cohesion")
    
    sil_df <- sil_plot$data
    sil_df$File <- basename(som_file)
    sil_data_list[[basename(som_file)]] <- sil_df
    
    ### Metodo della statistica Gap
    # Nota: nboot rimane 5 per velocizzare i test (il valore raccomandato è 500 per robustezza)
    gap_plot <- fviz_nbclust(
        som_values, 
        kmeans, 
        method   = "gap_stat", 
        k.max    = kmax,
        nboot    = 500, 
        iter.max = iter_max, 
        nstart   = nstart
    ) +
        labs(title    = paste("Gap Statistic Method -", basename(som_file)),
             subtitle = "Determining optimal clusters")
    
    gap_df <- gap_plot$data
    gap_df$File <- basename(som_file)
    gap_data_list[[basename(som_file)]] <- gap_df
}

# Combina i dati raccolti in un unico data frame per ciascun metodo
elbow_data <- do.call(rbind, elbow_data_list)
sil_data   <- do.call(rbind, sil_data_list)
gap_data   <- do.call(rbind, gap_data_list)

# Creazione del grafico finale combinato per il metodo del gomito
p_elbow <- ggplot(elbow_data, aes(x = clusters, y = y, group = File, color = File)) +
    geom_line() +
    geom_point() +
    labs(title = "Elbow Method - Combinato",
         x     = "Numero di cluster",
         y     = "Total within-clusters sum of squares") +
    theme_minimal()

# Creazione del grafico finale combinato per il metodo della silhouette
p_sil <- ggplot(sil_data, aes(x = clusters, y = y, group = File, color = File)) +
    geom_line() +
    geom_point() +
    labs(title = "Silhouette Method - Combinato",
         x     = "Numero di cluster",
         y     = "Average silhouette width") +
    theme_minimal()

# Creazione del grafico finale combinato per il metodo della statistica Gap
# Nota: qui usiamo y = gap, in quanto il data frame di gap_stat contiene tale colonna
p_gap <- ggplot(gap_data, aes(x = clusters, y = gap, group = File, color = File)) +
    geom_line() +
    geom_point() +
    labs(title = "Gap Statistic Method - Combinato",
         x     = "Numero di cluster",
         y     = "Gap statistic") +
    theme_minimal()

# Visualizzazione dei tre grafici finali
print(p_elbow)
print(p_sil)
print(p_gap)












#indice di dunn tra 3 e 6 reiterato con sovrapposizione grafici
# Caricamento delle librerie necessarie
library(kohonen)    # Per la Self-Organizing Map
library(cluster)    # Per metriche di clustering
library(clValid)    # Per il calcolo dell'indice di Dunn
library(ggplot2)    # Per la composizione dei grafici finali

# Definizione della cartella contenente i modelli SOM
som_folder <- "C:/PN_Sila/som_Sila/"

# Ottieni tutti i file .rds presenti nella cartella
som_files <- list.files(som_folder, pattern = "\\.rds$", full.names = TRUE)

# Inizializza la lista per raccogliere i dati dell'indice di Dunn
dunn_data_list <- list()

# Parametri aggiuntivi per kmeans
kmax      <- 6  # Limite massimo di cluster
iter_max  <- 100  # Numero massimo di iterazioni
nstart    <- 25   # Numero di inizializzazioni casuali

# Ciclo su tutti i file SOM presenti nella cartella
for (som_file in som_files) {
    
    # Stampa il nome del file in analisi
    cat("\nAnalizzando il file:", som_file, "\n")
    
    # Caricamento del modello SOM
    som_model <- readRDS(som_file)
    
    # Estrazione dei codici della SOM (matrice dei centroidi)
    som_values <- som_model$codes[[1]]
    
    # Creazione di un data frame per memorizzare i valori di Dunn
    dunn_results <- data.frame(clusters = integer(), dunn_index = numeric())
    
    for (k in 3:kmax) {
        set.seed(123)  # Per risultati riproducibili
        kmeans_result <- kmeans(som_values, centers = k, iter.max = iter_max, nstart = nstart)
        dunn_index <- dunn(dist(som_values), kmeans_result$cluster)
        dunn_results <- rbind(dunn_results, data.frame(clusters = k, dunn_index = dunn_index))
    }
    
    dunn_results$File <- basename(som_file)
    dunn_data_list[[basename(som_file)]] <- dunn_results
}

# Combina i dati raccolti in un unico data frame
dunn_data <- do.call(rbind, dunn_data_list)

# Creazione del grafico finale combinato per l'indice di Dunn
p_dunn <- ggplot(dunn_data, aes(x = clusters, y = dunn_index, group = File, color = File)) +
    geom_line() +
    geom_point() +
    labs(title = "Dunn Index - Combinato",
         x     = "Numero di cluster",
         y     = "Dunn Index Value") +
    theme_minimal()

# Visualizzazione del grafico
print(p_dunn)

