library(terra)
library(ggplot2)
library(cluster)
library(mclust)
library(factoextra)

# Imposta la directory di lavoro
setwd("C:/esame/falsi colori sila")

# Carica l'immagine NIR
nir_1984 <- rast("NIR_PN_SILA_84.TIF")

# Converti l'immagine in un vettore di valori pixel e rimuovi i NA
nir_vector <- na.omit(as.vector(nir_1984[]))

# Prendi un campione casuale di 20.000 pixel
set.seed(42)  # Per riproducibilità
sample_size <- 20000
sample_indices <- sample(seq_along(nir_vector), sample_size)
sample_vector <- nir_vector[sample_indices]


#GOMITO
# Calcola WCSS per diversi numeri di cluster
wcss <- sapply(1:10, function(k) {
  kmeans(sample_vector, centers = k, nstart = 10)$tot.withinss
})

# Visualizza il metodo dell'Elbow
plot(1:10, wcss, type = "b", pch = 19, frame = FALSE,
     xlab = "Numero di cluster", ylab = "Within-Cluster Sum of Squares (WCSS)",
     main = "Metodo dell'Elbow per Determinare il Numero Ottimale di Cluster")


#silhouette

library(cluster)  # Assicurati di avere il pacchetto cluster installato

# Calcola l'indice di silhouette per diversi numeri di cluster
sil_width <- sapply(3:10, function(k) {
  km <- kmeans(sample_vector, centers = k, nstart = 20)
  ss <- silhouette(km$cluster, dist(sample_vector))
  mean(ss[, 3])  # Media dell'indice di silhouette
})

# Identifica il numero di cluster con il massimo indice di silhouette
optimal_clusters <- which.max(sil_width) + 2  # +1 perché l'indice parte da 2 cluster

# Visualizza l'indice di silhouette
plot(3:10, sil_width, type = "b", pch = 19, frame = FALSE,
     xlab = "Numero di cluster", ylab = "Silhouette Width",
     main = "Indice di Silhouette per Determinare il Numero Ottimale di Cluster")

# Stampa il numero ottimale di cluster
cat("Il numero ottimale di cluster basato sull'indice di silhouette è:", optimal_clusters, "\n")


# Bayesian Information Criterion (BIC) con Gaussian Mixture Models (GMM)

library(mclust)

# Applica Gaussian Mixture Models usando mclust
gmm_model <- Mclust(sample_vector)

# Visualizza il risultato e il BIC
plot(gmm_model, what = "BIC", main = "BIC per Gaussian Mixture Models")

# Il numero ottimale di cluster
optimal_clusters_gmm <- gmm_model$G
print(paste("Numero ottimale di cluster secondo GMM:", optimal_clusters_gmm))

# GAP STATISTIC pare essere un po troppo fragile, solo 1000 pixel... 9 cluster forse?
library(factoextra)

# Impostazioni generali
set.seed(42)
sample_size <- 1000

# Estrai un campione di 1000 pixel dal tuo dataset
sample_indices <- sample(seq_along(nir_vector), sample_size)
sample_vector <- nir_vector[sample_indices]

# Applica Gap Statistic usando k-means
gap_stat <- clusGap(as.matrix(sample_vector), FUN = kmeans, nstart = 10, K.max = 10, B = 50)

# Visualizza il Gap Statistic con titolo personalizzato
fviz_gap_stat(gap_stat) + 
  ggtitle("Gap Statistic per Determinare il Numero Ottimale di Cluster")


# INDICE  DI DUMM
library(cluster)  # Per la funzione da usare per calcolare la distanza
library(factoextra)  # Per la visualizzazione

# Impostazioni generali
set.seed(42)
sample_size <- 1000

# Estrai un campione di 1000 pixel dal tuo dataset
sample_indices <- sample(seq_along(nir_vector), sample_size)
sample_vector <- nir_vector[sample_indices]

# Converti il campione in matrice
sample_matrix <- as.matrix(sample_vector)

# Funzione per calcolare l'indice di Dunn
dunn_index <- function(k, data) {
  km <- kmeans(data, centers = k, nstart = 10)
  cluster_data <- data.frame(cluster = km$cluster, data)
  
  # Calcola la distanza tra i punti
  dist_matrix <- dist(data)
  
  # Calcola la distanza intra-cluster
  intra_cluster_dist <- sapply(unique(cluster_data$cluster), function(cluster_id) {
    cluster_points <- data[cluster_data$cluster == cluster_id, , drop = FALSE]
    if (nrow(cluster_points) > 1) {
      return(max(dist(cluster_points)))
    } else {
      return(0)
    }
  })
  
  max_intra_cluster_dist <- max(intra_cluster_dist)
  
  # Calcola la distanza inter-cluster
  cluster_centers <- km$centers
  inter_cluster_dist <- as.matrix(dist(cluster_centers))
  min_inter_cluster_dist <- min(inter_cluster_dist[upper.tri(inter_cluster_dist)])
  
  # Calcola l'indice di Dunn
  if (max_intra_cluster_dist == 0) {
    return(0)  # Evita divisione per zero
  } else {
    return(min_inter_cluster_dist / max_intra_cluster_dist)
  }
}

# Intervallo dei cluster da 3 a 10
k_values <- 3:10

# Calcola l'indice di Dunn per ogni valore di k
dunn_values <- sapply(k_values, function(k) dunn_index(k, sample_matrix))

# Visualizza l'indice di Dunn
plot(k_values, dunn_values, type = "b", pch = 19, frame = FALSE,
     xlab = "Numero di cluster", ylab = "Indice di Dunn",
     main = "Indice di Dunn per Determinare il Numero Ottimale di Cluster")

# Trova il numero ottimale di cluster
optimal_k_dunn <- k_values[which.max(dunn_values)]
cat("Numero ottimale di cluster secondo l'indice di Dunn:", optimal_k_dunn, "\n")













#ciclo for per siluette, 20 prove con set.seed variabile, risultato con 9000 pixel è 5

library(cluster)  # Assicurati di avere il pacchetto cluster installato

# Impostazioni generali
num_iterations <- 20
sample_size <- 9000
silhouette_results <- matrix(NA, nrow = num_iterations, ncol = 8)  # 8 colonne per cluster da 3 a 10

for (i in 1:num_iterations) {
  set.seed(i)  # Cambia il seed ad ogni iterazione per la riproducibilità
  # Prendi un campione casuale di 9000 pixel
  sample_indices <- sample(seq_along(nir_vector), sample_size)
  sample_vector <- nir_vector[sample_indices]
  
  # Calcola l'indice di silhouette per diversi numeri di cluster (da 3 a 10)
  sil_width <- sapply(3:10, function(k) {
    km <- kmeans(sample_vector, centers = k, nstart = 10)
    ss <- silhouette(km$cluster, dist(sample_vector))
    mean(ss[, 3])  # Media dell'indice di silhouette
  })
  
  # Salva i risultati in una matrice
  silhouette_results[i, ] <- sil_width
}

# Calcola la media dell'indice di silhouette per ciascun numero di cluster
mean_silhouette <- colMeans(silhouette_results)

# Identifica il numero di cluster con il massimo valore medio di silhouette
optimal_clusters <- which.max(mean_silhouette) + 2  # +2 perché partiamo da 3 cluster

# Visualizza i risultati
plot(3:10, mean_silhouette, type = "b", pch = 19, frame = FALSE,
     xlab = "Numero di cluster", ylab = "Silhouette Width Medio",
     main = "Indice di Silhouette Medio per Determinare il Numero Ottimale di Cluster")

# Stampa il numero ottimale di cluster
cat("Il numero ottimale di cluster basato sull'indice di silhouette medio è:", optimal_clusters, "\n")







#ciclo per BIC con 100.000 pixel, 20 CICLI MEDIA 8.55

library(mclust)

# Impostazioni generali
num_iterations <- 20
sample_size <- 100000
optimal_clusters_list <- numeric(num_iterations)  # Vettore per memorizzare i cluster ottimali per ogni iterazione

# Ciclo for per eseguire il clustering con Gaussian Mixture Models (GMM) per diversi seed
for (i in 1:num_iterations) {
  set.seed(i)  # Cambia il seed ad ogni iterazione per la riproducibilità
  
  # Prendi un campione casuale di 100.000 pixel
  sample_indices <- sample(seq_along(nir_vector), sample_size)
  sample_vector <- nir_vector[sample_indices]
  
  # Applica Gaussian Mixture Models usando mclust
  gmm_model <- Mclust(sample_vector)
  
  # Salva il numero ottimale di cluster per questa iterazione
  optimal_clusters_list[i] <- gmm_model$G
}

# Calcola la media del numero ottimale di cluster
mean_optimal_clusters <- mean(optimal_clusters_list)

# Visualizza la media del numero ottimale di cluster
cat("Numero medio ottimale di cluster secondo GMM:", mean_optimal_clusters, "\n")



# altro indice Calinski-Harabasz più o meno anche 6/8 cluster

# Carica le librerie necessarie
library(cluster)
library(factoextra)

# Impostazioni generali
set.seed(42)
sample_size <- 1000

# Estrai un campione di 1000 pixel dal tuo dataset
sample_indices <- sample(seq_along(nir_vector), sample_size)
sample_vector <- nir_vector[sample_indices]

# Converti il campione in matrice
sample_matrix <- as.matrix(sample_vector)

# Funzione per calcolare l'indice di Calinski-Harabasz
calinski_harabasz_index <- function(k, data) {
  km <- kmeans(data, centers = k, nstart = 10)
  # Calcola l'indice di Calinski-Harabasz
  # Nota: calinhara non è disponibile direttamente, quindi usiamo la formula:
  cluster::clusGap(data, FUN = kmeans, K.max = k, B = 50)$Tab[which.max(cluster::clusGap(data, FUN = kmeans, K.max = k, B = 50)$Tab[, "gap"]), "gap"]
}

# Definisci l'intervallo dei cluster da 3 a 10
k_values <- 3:10

# Calcola l'indice di Calinski-Harabasz per ciascun numero di cluster
ch_values <- sapply(k_values, function(k) calinski_harabasz_index(k, sample_matrix))

# Visualizza l'indice di Calinski-Harabasz
plot(k_values, ch_values, type = "b", pch = 19, frame = FALSE,
     xlab = "Numero di cluster", ylab = "Indice di Calinski-Harabasz",
     main = "Indice di Calinski-Harabasz per Determinare il Numero Ottimale di Cluster")







# monte carlo siluette 4/5 cluster

# Imposta la working directory
setwd("C:/esame/falsi colori sila")

# Carica le librerie necessarie
library(terra)  # Per la manipolazione dei raster
library(cluster)  # Per il clustering e il calcolo dell'indice di silhouette
library(ggplot2)  # Per la visualizzazione
library(dplyr)  # Per la manipolazione dei dati

# Carica l'immagine NIR
nir_1984 <- rast("NIR_PN_SILA_84.TIF")

# Converti il raster in un vettore
nir_vector <- values(nir_1984, na.rm = TRUE)

# Imposta il campionamento
set.seed(42)  # Per la riproducibilità
sample_size <- 1000  # Numero di pixel da campionare
sample_indices <- sample(seq_along(nir_vector), sample_size)
sample_vector <- nir_vector[sample_indices]

# Converti il campione in una matrice (necessario per il clustering)
sample_matrix <- as.matrix(sample_vector)

# Funzione per calcolare l'indice di silhouette medio per una simulazione
calculate_silhouette_mean <- function(k, data, num_simulations) {
  silhouette_means <- numeric(num_simulations)
  for (i in 1:num_simulations) {
    # Genera dati sintetici basati sui dati originali
    synthetic_data <- data + matrix(rnorm(n = length(data), sd = 0.1), nrow = length(data), ncol = 1)
    km <- kmeans(synthetic_data, centers = k, nstart = 10)
    sil <- silhouette(km$cluster, dist(synthetic_data))
    silhouette_means[i] <- mean(sil[, "sil_width"])
  }
  return(mean(silhouette_means))
}

# Definisci l'intervallo dei cluster da 3 a 10
k_values <- 3:10

# Numero di simulazioni Monte Carlo
num_simulations <- 100

# Calcola l'indice di silhouette medio per ciascun numero di cluster
silhouette_results <- sapply(k_values, function(k) calculate_silhouette_mean(k, sample_matrix, num_simulations))

# Visualizza i risultati
plot(k_values, silhouette_results, type = "b", pch = 19, frame = FALSE,
     xlab = "Numero di cluster", ylab = "Indice di Silhouette Medio",
     main = "Indice di Silhouette Medio per Diversi Numeri di Cluster (Monte Carlo)")




#ALTRO BIC Codice di Esempio per Gaussian Mixture Models (GMM)

# Carica le librerie necessarie
library(mclust)

# Imposta il seme per la riproducibilità
set.seed(42)

# Estrai un campione di 1000 pixel dal tuo dataset
sample_size <- 1000
sample_indices <- sample(seq_along(nir_vector), sample_size)
sample_vector <- nir_vector[sample_indices]
sample_matrix <- matrix(sample_vector, ncol = 1)

# Applica Gaussian Mixture Models
gmm_model <- Mclust(sample_matrix, G = 1:10)  # Testa per un numero di cluster tra 1 e 10

# Visualizza il risultato e il BIC
plot(gmm_model, what = "BIC", main = "BIC per Gaussian Mixture Models")

# Numero ottimale di cluster secondo GMM
optimal_clusters_gmm <- gmm_model$G
print(paste("Numero ottimale di cluster secondo GMM:", optimal_clusters_gmm))


#LINKAGE CLUSTERING GEERARCHICO 

# Carica le librerie necessarie
library(terra)
library(cluster)
library(ggplot2)

# Imposta la directory di lavoro e carica l'immagine NIR
setwd("C:/esame/falsi colori sila")
nir_1984 <- rast("NIR_PN_SILA_84.TIF")

# Estrai i dati NIR come vettore e campiona 1000 pixel
nir_vector <- values(nir_1984, na.rm = TRUE)
sample_size <- 1000
set.seed(42)  # Per riproducibilità
sample_indices <- sample(seq_along(nir_vector), sample_size)
sample_vector <- nir_vector[sample_indices]
sample_matrix <- matrix(sample_vector, ncol = 1)

# Normalizza i dati (opzionale)
sample_matrix <- scale(sample_matrix)

# Calcola la matrice di distanza
dist_matrix <- dist(sample_matrix)

# Esegui l'Hierarchical Clustering con linkage completo
hc_complete <- hclust(dist_matrix, method = "complete")

# Visualizza il dendrogramma
plot(hc_complete, main = "Dendrogramma del Clustering Gerarchico (Linkage Completo)", xlab = "", sub = "", cex = 0.9)

# Taglia l'albero per ottenere un numero specifico di cluster
k <- 4  # Numero di cluster desiderato
clusters <- cutree(hc_complete, k)

# Aggiungi i risultati dei cluster al dataframe
hierarchical_df <- data.frame(
  pixel_value = sample_vector,
  cluster = factor(clusters),
  y = rep(1, length(sample_vector))  # Asse y costante per visualizzazione
)

# Visualizza i cluster con ggplot
ggplot(hierarchical_df, aes(x = pixel_value, y = y, color = cluster)) +
  geom_point(alpha = 0.7) +
  labs(title = "Cluster con Clustering Gerarchico (Linkage Completo)",
       x = "Valore NIR",
       y = "Cluster") +
  theme_minimal() +
  scale_color_discrete(name = "Cluster")





# CLUSTERING SPETTRALE
# Carica le librerie necessarie
library(terra)
library(kernlab)
library(ggplot2)

# Imposta la directory di lavoro e carica l'immagine NIR
setwd("C:/esame/falsi colori sila")
nir_1984 <- rast("NIR_PN_SILA_84.TIF")

# Estrai i dati NIR come vettore e campiona 1000 pixel
nir_vector <- values(nir_1984, na.rm = TRUE)
sample_size <- 1000
set.seed(42)  # Per riproducibilità
sample_indices <- sample(seq_along(nir_vector), sample_size)
sample_vector <- nir_vector[sample_indices]
sample_matrix <- matrix(sample_vector, ncol = 1)

# Normalizza i dati (opzionale)
sample_matrix <- scale(sample_matrix)

# Calcola la matrice di similarità (kernel gaussiano)
sigma <- 0.5
dist_matrix <- as.matrix(dist(sample_matrix))
similarity_matrix <- exp(-dist_matrix^2 / (2 * sigma^2))

# Applica il clustering spettrale
num_clusters <- 4  # Numero di cluster desiderato
spectral_result <- specc(similarity_matrix, centers = num_clusters)

# Crea un dataframe per i risultati del clustering spettrale
spectral_df <- data.frame(
  pixel_value = sample_vector,
  cluster = factor(spectral_result@.Data),
  y = rep(1, length(sample_vector))  # Asse y costante per visualizzazione
)

# Visualizza i cluster con ggplot
ggplot(spectral_df, aes(x = pixel_value, y = y, color = cluster)) +
  geom_point(alpha = 0.7) +
  labs(title = "Cluster con Spectral Clustering",
       x = "Valore NIR",
       y = "Cluster") +
  theme_minimal() +
  scale_color_discrete(name = "Cluster")




#GERARCHICO, conoscendo già cluster numero 30K 5 cluster
# Carica le librerie necessarie
library(terra)
library(cluster)
library(ggplot2)

# Imposta la directory di lavoro e carica l'immagine NIR
setwd("C:/esame/falsi colori sila")
nir_1984 <- rast("NIR_PN_SILA_84.TIF")

# Estrai i dati NIR come vettore e campiona 1000 pixel
nir_vector <- values(nir_1984, na.rm = TRUE)
sample_size <- 1000
set.seed(42)  # Per riproducibilità
sample_indices <- sample(seq_along(nir_vector), sample_size)
sample_vector <- nir_vector[sample_indices]
sample_matrix <- matrix(sample_vector, ncol = 1)

# Normalizza i dati (opzionale)
sample_matrix <- scale(sample_matrix)

# Calcola la matrice di distanza
dist_matrix <- dist(sample_matrix)

# Esegui l'Hierarchical Clustering con linkage completo
hc_complete <- hclust(dist_matrix, method = "complete")

# Visualizza il dendrogramma
plot(hc_complete, main = "Dendrogramma del Clustering Gerarchico (Linkage Completo)", xlab = "", sub = "", cex = 0.9)

# Taglia l'albero per ottenere un numero specifico di cluster
k <- 4  # Numero di cluster desiderato
clusters <- cutree(hc_complete, k)

# Aggiungi i risultati dei cluster al dataframe
hierarchical_df <- data.frame(
  pixel_value = sample_vector,
  cluster = factor(clusters),
  y = rep(1, length(sample_vector))  # Asse y costante per visualizzazione
)

# Visualizza i cluster con ggplot
ggplot(hierarchical_df, aes(x = pixel_value, y = y, color = cluster)) +
  geom_point(alpha = 0.7) +
  labs(title = "Cluster con Clustering Gerarchico (Linkage Completo)",
       x = "Valore NIR",
       y = "Cluster") +
  theme_minimal() +
  scale_color_discrete(name = "Cluster")
