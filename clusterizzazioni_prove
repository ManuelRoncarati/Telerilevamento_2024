library(terra)
library(ggplot2)
library(cluster)
library(mclust)
library(factoextra)

# Imposta la directory di lavoro
setwd("C:/esame/falsi colori sila")

# Carica l'immagine NIR
nir_1984 <- rast("NIR_PN_SILA_84.TIF")

# Converti l'immagine in un vettore di valori pixel e rimuovi i NA
nir_vector <- na.omit(as.vector(nir_1984[]))

# Prendi un campione casuale di 20.000 pixel
set.seed(42)  # Per riproducibilità
sample_size <- 20000
sample_indices <- sample(seq_along(nir_vector), sample_size)
sample_vector <- nir_vector[sample_indices]


#GOMITO
# Calcola WCSS per diversi numeri di cluster
wcss <- sapply(1:10, function(k) {
  kmeans(sample_vector, centers = k, nstart = 10)$tot.withinss
})

# Visualizza il metodo dell'Elbow
plot(1:10, wcss, type = "b", pch = 19, frame = FALSE,
     xlab = "Numero di cluster", ylab = "Within-Cluster Sum of Squares (WCSS)",
     main = "Metodo dell'Elbow per Determinare il Numero Ottimale di Cluster")


#silhouette

library(cluster)  # Assicurati di avere il pacchetto cluster installato

# Calcola l'indice di silhouette per diversi numeri di cluster
sil_width <- sapply(3:10, function(k) {
  km <- kmeans(sample_vector, centers = k, nstart = 20)
  ss <- silhouette(km$cluster, dist(sample_vector))
  mean(ss[, 3])  # Media dell'indice di silhouette
})

# Identifica il numero di cluster con il massimo indice di silhouette
optimal_clusters <- which.max(sil_width) + 2  # +1 perché l'indice parte da 2 cluster

# Visualizza l'indice di silhouette
plot(3:10, sil_width, type = "b", pch = 19, frame = FALSE,
     xlab = "Numero di cluster", ylab = "Silhouette Width",
     main = "Indice di Silhouette per Determinare il Numero Ottimale di Cluster")

# Stampa il numero ottimale di cluster
cat("Il numero ottimale di cluster basato sull'indice di silhouette è:", optimal_clusters, "\n")


# Bayesian Information Criterion (BIC) con Gaussian Mixture Models (GMM)

library(mclust)

# Applica Gaussian Mixture Models usando mclust
gmm_model <- Mclust(sample_vector)

# Visualizza il risultato e il BIC
plot(gmm_model, what = "BIC", main = "BIC per Gaussian Mixture Models")

# Il numero ottimale di cluster
optimal_clusters_gmm <- gmm_model$G
print(paste("Numero ottimale di cluster secondo GMM:", optimal_clusters_gmm))

# GAP STATISTIC pare essere un po troppo fragile, solo 1000 pixel... 9 cluster forse?
library(factoextra)

# Impostazioni generali
set.seed(42)
sample_size <- 1000

# Estrai un campione di 1000 pixel dal tuo dataset
sample_indices <- sample(seq_along(nir_vector), sample_size)
sample_vector <- nir_vector[sample_indices]

# Applica Gap Statistic usando k-means
gap_stat <- clusGap(as.matrix(sample_vector), FUN = kmeans, nstart = 10, K.max = 10, B = 50)

# Visualizza il Gap Statistic con titolo personalizzato
fviz_gap_stat(gap_stat) + 
  ggtitle("Gap Statistic per Determinare il Numero Ottimale di Cluster")


# INDICE  DI DUMM
library(cluster)  # Per la funzione da usare per calcolare la distanza
library(factoextra)  # Per la visualizzazione

# Impostazioni generali
set.seed(42)
sample_size <- 1000

# Estrai un campione di 1000 pixel dal tuo dataset
sample_indices <- sample(seq_along(nir_vector), sample_size)
sample_vector <- nir_vector[sample_indices]

# Converti il campione in matrice
sample_matrix <- as.matrix(sample_vector)

# Funzione per calcolare l'indice di Dunn
dunn_index <- function(k, data) {
  km <- kmeans(data, centers = k, nstart = 10)
  cluster_data <- data.frame(cluster = km$cluster, data)
  
  # Calcola la distanza tra i punti
  dist_matrix <- dist(data)
  
  # Calcola la distanza intra-cluster
  intra_cluster_dist <- sapply(unique(cluster_data$cluster), function(cluster_id) {
    cluster_points <- data[cluster_data$cluster == cluster_id, , drop = FALSE]
    if (nrow(cluster_points) > 1) {
      return(max(dist(cluster_points)))
    } else {
      return(0)
    }
  })
  
  max_intra_cluster_dist <- max(intra_cluster_dist)
  
  # Calcola la distanza inter-cluster
  cluster_centers <- km$centers
  inter_cluster_dist <- as.matrix(dist(cluster_centers))
  min_inter_cluster_dist <- min(inter_cluster_dist[upper.tri(inter_cluster_dist)])
  
  # Calcola l'indice di Dunn
  if (max_intra_cluster_dist == 0) {
    return(0)  # Evita divisione per zero
  } else {
    return(min_inter_cluster_dist / max_intra_cluster_dist)
  }
}

# Intervallo dei cluster da 3 a 10
k_values <- 3:10

# Calcola l'indice di Dunn per ogni valore di k
dunn_values <- sapply(k_values, function(k) dunn_index(k, sample_matrix))

# Visualizza l'indice di Dunn
plot(k_values, dunn_values, type = "b", pch = 19, frame = FALSE,
     xlab = "Numero di cluster", ylab = "Indice di Dunn",
     main = "Indice di Dunn per Determinare il Numero Ottimale di Cluster")

# Trova il numero ottimale di cluster
optimal_k_dunn <- k_values[which.max(dunn_values)]
cat("Numero ottimale di cluster secondo l'indice di Dunn:", optimal_k_dunn, "\n")













#ciclo for per siluette, 20 prove con set.seed variabile, risultato con 9000 pixel è 5

library(cluster)  # Assicurati di avere il pacchetto cluster installato

# Impostazioni generali
num_iterations <- 20
sample_size <- 9000
silhouette_results <- matrix(NA, nrow = num_iterations, ncol = 8)  # 8 colonne per cluster da 3 a 10

for (i in 1:num_iterations) {
  set.seed(i)  # Cambia il seed ad ogni iterazione per la riproducibilità
  # Prendi un campione casuale di 9000 pixel
  sample_indices <- sample(seq_along(nir_vector), sample_size)
  sample_vector <- nir_vector[sample_indices]
  
  # Calcola l'indice di silhouette per diversi numeri di cluster (da 3 a 10)
  sil_width <- sapply(3:10, function(k) {
    km <- kmeans(sample_vector, centers = k, nstart = 10)
    ss <- silhouette(km$cluster, dist(sample_vector))
    mean(ss[, 3])  # Media dell'indice di silhouette
  })
  
  # Salva i risultati in una matrice
  silhouette_results[i, ] <- sil_width
}

# Calcola la media dell'indice di silhouette per ciascun numero di cluster
mean_silhouette <- colMeans(silhouette_results)

# Identifica il numero di cluster con il massimo valore medio di silhouette
optimal_clusters <- which.max(mean_silhouette) + 2  # +2 perché partiamo da 3 cluster

# Visualizza i risultati
plot(3:10, mean_silhouette, type = "b", pch = 19, frame = FALSE,
     xlab = "Numero di cluster", ylab = "Silhouette Width Medio",
     main = "Indice di Silhouette Medio per Determinare il Numero Ottimale di Cluster")

# Stampa il numero ottimale di cluster
cat("Il numero ottimale di cluster basato sull'indice di silhouette medio è:", optimal_clusters, "\n")







#ciclo per BIC con 100.000 pixel, 20 CICLI MEDIA 8.55

library(mclust)

# Impostazioni generali
num_iterations <- 20
sample_size <- 100000
optimal_clusters_list <- numeric(num_iterations)  # Vettore per memorizzare i cluster ottimali per ogni iterazione

# Ciclo for per eseguire il clustering con Gaussian Mixture Models (GMM) per diversi seed
for (i in 1:num_iterations) {
  set.seed(i)  # Cambia il seed ad ogni iterazione per la riproducibilità
  
  # Prendi un campione casuale di 100.000 pixel
  sample_indices <- sample(seq_along(nir_vector), sample_size)
  sample_vector <- nir_vector[sample_indices]
  
  # Applica Gaussian Mixture Models usando mclust
  gmm_model <- Mclust(sample_vector)
  
  # Salva il numero ottimale di cluster per questa iterazione
  optimal_clusters_list[i] <- gmm_model$G
}

# Calcola la media del numero ottimale di cluster
mean_optimal_clusters <- mean(optimal_clusters_list)

# Visualizza la media del numero ottimale di cluster
cat("Numero medio ottimale di cluster secondo GMM:", mean_optimal_clusters, "\n")



# altro indice

# Funzione per calcolare l'indice di Calinski-Harabasz
calinski_harabasz_index <- function(k, data) {
  km <- kmeans(data, centers = k, nstart = 10)
  # Calcola l'indice di Calinski-Harabasz
  # calinhara non è disponibile, quindi usiamo la formula manuale:
  cluster::clusGap(data, FUN = kmeans, K.max = k, B = 50)$Tab[which.max(cluster::clusGap(data, FUN = kmeans, K.max = k, B = 50)$Tab[, "gap"]), "gap"]
}

# Definisci l'intervallo dei cluster da 3 a 10
k_values <- 3:10

# Calcola l'indice di Calinski-Harabasz per ciascun numero di cluster
ch_values <- sapply(k_values, function(k) calinski_harabasz_index(k, sample_matrix))

# Visualizza l'indice di Calinski-Harabasz
plot(k_values, ch_values, type = "b", pch = 19, frame = FALSE,
     xlab = "Numero di cluster", ylab = "Indice di Calinski-Harabasz",
     main = "Indice di Calinski-Harabasz per Determinare il Numero Ottimale di Cluster")


